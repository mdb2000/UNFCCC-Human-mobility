{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mdb2000/UNFCCC-Human-mobility/blob/Python-codes/Dataset_sentence.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8737828f-a95f-4e1f-8a92-8cc80d8bc6da",
      "metadata": {
        "id": "8737828f-a95f-4e1f-8a92-8cc80d8bc6da"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import nlp\n",
        "folder= 'COP decisions'\n",
        "abbr = 'COP'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a6dd60e-4efc-4f12-bbcc-73529670ec02",
      "metadata": {
        "id": "2a6dd60e-4efc-4f12-bbcc-73529670ec02"
      },
      "outputs": [],
      "source": [
        "# Increase the max_length limit\n",
        "nlp.max_length = 1000000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9956262f-b7a2-4687-8534-a67a1bdad1cf",
      "metadata": {
        "id": "9956262f-b7a2-4687-8534-a67a1bdad1cf"
      },
      "outputs": [],
      "source": [
        "# Load the spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9af2545-00c9-4823-9211-b7653c2b6feb",
      "metadata": {
        "id": "c9af2545-00c9-4823-9211-b7653c2b6feb"
      },
      "outputs": [],
      "source": [
        "# Define the keywords to look for\n",
        "keywords = {'migration', 'displacement', 'immigration', 'relocation', 'refugee', 'migrant'}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "215cca80-162c-4457-ba98-9ad10719886c",
      "metadata": {
        "id": "215cca80-162c-4457-ba98-9ad10719886c"
      },
      "outputs": [],
      "source": [
        "folder_path = f'C:\\\\Users\\\\3104470\\\\Desktop\\\\data\\\\text extraction\\\\{folder}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4bc226a4-8f9e-4eab-a5f4-5ca864be80f7",
      "metadata": {
        "id": "4bc226a4-8f9e-4eab-a5f4-5ca864be80f7"
      },
      "outputs": [],
      "source": [
        "# Function to process documents\n",
        "def process_documents(folder_path, keywords):\n",
        "    data = []\n",
        "    doc_id = 1\n",
        "\n",
        "    # Get list of files\n",
        "    files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
        "\n",
        "    # Iterate over files with a progress bar\n",
        "    for filename in tqdm(files, desc=\"Processing Documents\"):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            # Check if text length exceeds the max_length\n",
        "            if len(text) > nlp.max_length:\n",
        "                chunks = [text[i:i + nlp.max_length] for i in range(0, len(text), nlp.max_length)]\n",
        "            else:\n",
        "                chunks = [text]\n",
        "\n",
        "            for chunk in chunks:\n",
        "                doc = nlp(chunk)\n",
        "                sentences = list(doc.sents)\n",
        "                for i, sent in enumerate(sentences):\n",
        "                    if any(keyword in sent.text for keyword in keywords):\n",
        "                        # Include the previous and next sentence for context\n",
        "                        prev_sent = sentences[i-1].text if i > 0 else ''\n",
        "                        next_sent = sentences[i+1].text if i < len(sentences) - 1 else ''\n",
        "                        combined_sent = f\"{prev_sent} {sent.text.strip()} {next_sent}\".strip()\n",
        "                        doc_id_str = f\"{doc_id}_{abbr}\"\n",
        "                        data.append({\"Document_ID\": doc_id_str, \"Sentence\": combined_sent})\n",
        "\n",
        "        doc_id += 1\n",
        "\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1bed671-87db-423c-b0b0-0d115da59c48",
      "metadata": {
        "id": "c1bed671-87db-423c-b0b0-0d115da59c48",
        "outputId": "a203b8ad-08cd-4a36-eacf-8bbdc5c842da"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Documents: 100%|████████████████████████████████████████████████████████████| 28/28 [01:46<00:00,  3.80s/it]\n"
          ]
        }
      ],
      "source": [
        "# Process the documents\n",
        "processed_data = process_documents(folder_path, keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d82bad51-1be9-4e24-9aa6-093d02572f2a",
      "metadata": {
        "id": "d82bad51-1be9-4e24-9aa6-093d02572f2a"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame\n",
        "df = pd.DataFrame(processed_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd8a7bf6-11b0-44c2-a038-b9f992d7f09d",
      "metadata": {
        "id": "fd8a7bf6-11b0-44c2-a038-b9f992d7f09d"
      },
      "outputs": [],
      "source": [
        "# Function to clean unnecessary spaces\n",
        "def clean_sentence(sentence):\n",
        "    # Remove URLs\n",
        "    url_pattern = re.compile(r'http[s]?://\\S+|www\\.\\S+')\n",
        "    sentence = re.sub(url_pattern, '', sentence)\n",
        "    # Remove FCCC patterns with optional \".1\"\n",
        "    fccc_pattern = re.compile(r'\\d*fccc/\\S+/\\d{4}/\\d+(\\.\\d+)?')\n",
        "    sentence = re.sub(fccc_pattern, '', sentence)\n",
        "    # Remove encoded file patterns\n",
        "    file_pattern = re.compile(r'\\S+%20\\S+')\n",
        "    sentence = re.sub(file_pattern, '', sentence)\n",
        "    # Remove other file patterns\n",
        "    other_file_pattern = re.compile(r'\\S+_\\S+\\.pdf|\\S+/\\S+\\.pdf')\n",
        "    sentence = re.sub(other_file_pattern, '', sentence)\n",
        "    # Remove URL-like patterns\n",
        "    url_like_pattern = re.compile(r'\\S+\\.\\S+/\\S+')\n",
        "    sentence = re.sub(url_like_pattern, '', sentence)\n",
        "    # Remove unnecessary spaces and dots\n",
        "    sentence = re.sub(r'\\s+', ' ', sentence).strip()\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f33ff6d-d83d-4c95-b5b8-c946a92194ce",
      "metadata": {
        "id": "8f33ff6d-d83d-4c95-b5b8-c946a92194ce"
      },
      "outputs": [],
      "source": [
        "# Remove specific unwanted characters\n",
        "def remove_unwanted_characters(sentence):\n",
        "    sentence = re.sub(r'[,.!?:;_\\-/\\\\â€%&<>@#+*â€˜™“\"ï]', '', sentence)\n",
        "    # Remove any remaining non-ASCII characters\n",
        "    sentence = re.sub(r'[^\\x00-\\x7F]+', '', sentence)\n",
        "    return sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afbf02ae-bfa7-4f0d-905e-6c5c29ede2fd",
      "metadata": {
        "id": "afbf02ae-bfa7-4f0d-905e-6c5c29ede2fd"
      },
      "outputs": [],
      "source": [
        "# Function to filter sentences with at least 32 characters\n",
        "def filter_sentence(sentence):\n",
        "    return len(sentence) >= 90"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3dbc152-acb5-4a45-afe7-1da36f66b356",
      "metadata": {
        "id": "f3dbc152-acb5-4a45-afe7-1da36f66b356"
      },
      "outputs": [],
      "source": [
        "# Add Sentence_ID column\n",
        "df['Sentence_ID'] = [f\"{abbr}_sent_{i+1}\" for i in range(len(df))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65f7f885-2fab-4321-821d-da178a78148f",
      "metadata": {
        "id": "65f7f885-2fab-4321-821d-da178a78148f"
      },
      "outputs": [],
      "source": [
        "# Apply cleaning to the 'Sentence' column\n",
        "df['Sentence'] = df['Sentence'].apply(clean_sentence)\n",
        "df['Sentence'] = df['Sentence'].apply(remove_unwanted_characters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1202d74-4219-492b-bd95-31c0d92c55b4",
      "metadata": {
        "id": "d1202d74-4219-492b-bd95-31c0d92c55b4"
      },
      "outputs": [],
      "source": [
        "# Filter sentences with at least 32 characters\n",
        "df = df[df['Sentence'].apply(filter_sentence)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "906fb3b0-c203-4998-afa6-bda318ca375e",
      "metadata": {
        "id": "906fb3b0-c203-4998-afa6-bda318ca375e"
      },
      "outputs": [],
      "source": [
        "# Save the cleaned DataFrame to a CSV file\n",
        "df.to_csv(f'C:/Users/3104470/Desktop/output/sentences datasets/df_clean_sentences_long_{folder}.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9329516-8931-4314-9490-16f3df715032",
      "metadata": {
        "id": "b9329516-8931-4314-9490-16f3df715032"
      },
      "outputs": [],
      "source": [
        "#now filter for english and remove duplicates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "938b025f-45c4-4e3d-9fb5-6f62b4fce1d4",
      "metadata": {
        "id": "938b025f-45c4-4e3d-9fb5-6f62b4fce1d4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from langdetect import detect, DetectorFactory\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "\n",
        "# Ensure consistent results from langdetect\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "\n",
        "\n",
        "# Define a function to detect if a sentence is in English\n",
        "def is_english(Sentence):\n",
        "    try:\n",
        "        return detect(Sentence) == 'en'\n",
        "    except:\n",
        "        return False\n",
        "\n",
        "# Filter sentences written in English\n",
        "df['is_english'] = df['Sentence'].apply(is_english)\n",
        "english_sentences = df[df['is_english']]\n",
        "\n",
        "# Remove duplicates\n",
        "unique_english_sentences = english_sentences.drop_duplicates(subset=['Sentence'])\n",
        "\n",
        "# Drop the 'is_english' column as it is no longer needed\n",
        "unique_english_sentences = unique_english_sentences.drop(columns=['is_english'])\n",
        "\n",
        "# Save the filtered dataset\n",
        "unique_english_sentences.to_csv(f'C:\\\\Users\\\\3104470\\\\Desktop\\\\output\\\\sentences datasets\\\\long version filtered eng&duplicates\\\\{folder}_filtered_dataset.csv', index=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60df29c0-4460-456e-9dc7-ffcb87fc6d30",
      "metadata": {
        "id": "60df29c0-4460-456e-9dc7-ffcb87fc6d30",
        "outputId": "2926bf89-8020-416c-a3f9-da6e16c8d1f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Combined CSV file has been saved to C:\\Users\\3104470\\Desktop\\output\\sentences datasets\\long version filtered eng&duplicates\\combined_final_datasets.csv\n"
          ]
        }
      ],
      "source": [
        "#put everything inside one single dataset\n",
        "# Define the folder path\n",
        "folder_path = 'C:\\\\Users\\\\3104470\\\\Desktop\\\\output\\\\sentences datasets\\\\long version filtered eng&duplicates'\n",
        "\n",
        "# Initialize an empty list to store individual dataframes\n",
        "dataframes = []\n",
        "\n",
        "# Loop through each file in the folder\n",
        "first_file = True\n",
        "for file_name in os.listdir(folder_path):\n",
        "    if file_name.endswith('.csv'):  # Ensure we only read CSV files\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        if first_file:\n",
        "            df = pd.read_csv(file_path)\n",
        "            first_file = False\n",
        "        else:\n",
        "            df = pd.read_csv(file_path, header=0)\n",
        "        dataframes.append(df)\n",
        "\n",
        "# Concatenate all dataframes into one\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Save the combined dataframe to a new CSV file\n",
        "output_file_path = os.path.join(folder_path, 'combined_final_datasets.csv')\n",
        "combined_df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(f\"Combined CSV file has been saved to {output_file_path}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}